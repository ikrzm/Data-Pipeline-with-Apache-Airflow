# Data Pipeline with Apache Airflow

## Description

This project implements a robust data pipeline using Apache Airflow, integrating PostgreSQL for data storage, Redis as a message broker, and MongoDB for NoSQL data handling. It's designed for scalability, efficiency, and ease of management, suitable for a wide range of data processing and ETL tasks.

## Features

- **Apache Airflow**: Orchestrates complex workflows and data processing tasks.
- **PostgreSQL**: Robust relational database for structured data storage.
- **MongoDB**: Flexible NoSQL database for unstructured data handling.
- **Docker Compose**: Simplifies deployment of multi-container applications.

## Prerequisites

- Docker and Docker Compose installed.
- Basic understanding of Python, SQL, and NoSQL databases.
- WSL2 installed

## Installation & Setup

1. **Clone the Repository**
   ```bash
   git clone [Your Repository URL]
   cd [Your Repository Name]
2. **Setup your evironement**
 
   
4.  **Build and Run with Docker Compose**
     ```bash
     docker compose up --build
This command builds the Docker images and starts the services defined in your docker-compose.yml file.
   
